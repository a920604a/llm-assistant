import glob
import os
import json
import click
import re
import requests
from tqdm.auto import tqdm
from typing import List
from prefect import flow, task
from langchain.text_splitter import RecursiveCharacterTextSplitter
from qdrant_client import models

from storage.qdrant import qdrant_client
from conf import OLLAMA_API_URL, COLLECTION_NAME
from services.workflow.embedding import get_embedding


# === LangChain ===
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800, chunk_overlap=150, separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " "]
)


# âœ… å»ºç«‹ Collectionï¼ˆè‹¥å°šæœªå»ºç«‹ï¼‰
@task
def ensure_qdrant_collection():
    if COLLECTION_NAME not in [
        c.name for c in qdrant_client.get_collections().collections
    ]:
        qdrant_client.recreate_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=384, distance=models.Distance.COSINE
            ),
        )


# âœ… ç¿»è­¯æ–‡å­—
@task
def ollama_translate(text: str, model: str = "gpt-oss:20b") -> str:
    response = requests.post(
        f"{OLLAMA_API_URL}/api/generate",
        json={
            "model": model,
            "prompt": f"è«‹å¹«æˆ‘å°‡ä»¥ä¸‹å…§å®¹ç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼š\n{text}",
            "stream": False,
        },
    )
    response.raise_for_status()
    return response.json()["response"]


def clean_json_string(s: str) -> str:
    # ç§»é™¤é–‹é ­çš„ ```json èˆ‡çµå°¾çš„ ```
    s = s.strip()
    s = re.sub(r"^```json\s*", "", s)
    s = re.sub(r"\s*```$", "", s)
    return s


# âœ… ç”¢å‡º Metadata
@task
def ollama_generate_metadata(text: str, model: str = "gpt-oss:20b") -> dict:
    meta_prompt = f"""
è«‹é–±è®€ä»¥ä¸‹æ•™å­¸ç­†è¨˜ï¼Œä¸¦åˆ†æå‡ºä»¥ä¸‹å±¬æ€§ï¼š
- ä¸»é¡Œåˆ†é¡ï¼ˆä»¥ä¸€æ®µæ–‡å­—æè¿°ï¼‰
- é©åˆç¨‹åº¦ï¼ˆåˆå­¸è€…ã€ä¸­éšã€é€²éšï¼‰
- ä¸‰å€‹é—œéµå­—

æ•™å­¸å…§å®¹å¦‚ä¸‹ï¼š
{text}
è«‹ç”¨ JSON æ ¼å¼è¼¸å‡ºï¼Œä¾‹å¦‚ï¼š
{{
  "title": "å¤§æ•¸æ“šæ¶æ§‹è¨­è¨ˆ",
  "level": "ä¸­éš",
  "keywords": ["æ•¸æ“šè½‰å‹", "åˆ†æ•£å¼å„²å­˜", "è³‡æ–™æ¹–"]
}}
"""
    response = requests.post(
        f"{OLLAMA_API_URL}/api/generate",
        json={
            "model": model,
            "prompt": meta_prompt,
            "stream": False,
        },
    )
    response.raise_for_status()
    raw = response.json()["response"]
    try:
        cleaned = clean_json_string(raw)
        return json.loads(cleaned)
    except Exception:
        print("âš ï¸ ç„¡æ³•è§£æ metadata JSONï¼Œå›å‚³ç‚ºï¼š", raw)
        return {"topic": "", "level": "", "keywords": []}


# âœ… ä¸»æµç¨‹ï¼šåŒ¯å…¥ç­†è¨˜
@flow(name="Import Markdown Notes")
def import_md_notes_flow(md_text_dict: dict):
    ensure_qdrant_collection()

    points = []
    idx = 0
    BATCH_SIZE = 1

    # for filename, md_text in md_text_dict.items():
    for filename, md_text in tqdm(
        md_text_dict.items(), total=len(md_text_dict), desc="è™•ç†æª”æ¡ˆ"
    ):

        print(f"â¡ï¸ è™•ç†æª”æ¡ˆï¼š{filename}ï¼ŒåŸå§‹å­—æ•¸: {len(md_text)}")
        translated = ollama_translate(md_text)
        # print(f"ç¿»è­¯çµæœ : {translated}")

        print("ğŸ§  ç”¢å‡º Metadata...")
        metadata = ollama_generate_metadata(translated)
        print(f"Metadata çµæœ : {metadata}")

        print("âœ‚ï¸ åˆ†æ®µä¸­...")
        chunks = text_splitter.split_text(translated)
        for chunk in chunks:
            # vector = embeddings.embed_query(chunk)
            vector = get_embedding(chunk)
            # print(vector.shape())
            payload = {
                "text": chunk,
                "translated": True,
                **metadata,  # title, level, keywords
            }
            points.append(models.PointStruct(id=idx, vector=vector, payload=payload))
            idx += 1
            # if len(points) >= BATCH_SIZE:
            #     qdrant_client.upsert(collection_name=COLLECTION_NAME, points=points)
            #     print(f"å¯«å…¥ {len(points)} ç­†è³‡æ–™")
            #     points = []  # æ¸…ç©ºå·²å¯«å…¥çš„ batch

    # å¯«å…¥æœ€å¾Œå‰©é¤˜çš„é»
    # if points:
    #     qdrant_client.upsert(collection_name=COLLECTION_NAME, points=points)
    #     print(f"å¯«å…¥æœ€å¾Œ {len(points)} ç­†è³‡æ–™")
    # print(f"ğŸ“¦ å¯«å…¥ {len(points)} ç­†è³‡æ–™åˆ° Qdrant")
    qdrant_client.upsert(collection_name=COLLECTION_NAME, points=points)


# âœ… CLI ä»‹é¢ä½¿ç”¨ click
@click.command()
@click.option(
    "--path",
    type=click.Path(exists=True, file_okay=False),
    required=True,
    help="Markdown è³‡æ–™å¤¾è·¯å¾‘",
)
def cli(path):
    all_texts = {}

    if os.path.isdir(path):
        # æ‰¾è³‡æ–™å¤¾å…§æ‰€æœ‰ .md æª”æ¡ˆè·¯å¾‘
        md_files = glob.glob(os.path.join(path, "*.md"))
        for file in md_files:
            with open(file, "r", encoding="utf-8") as f:
                all_texts[file] = f.read()

    elif os.path.isfile(path) and path.endswith(".md"):
        # å–®ä¸€ .md æª”æ¡ˆ
        with open(path, "r", encoding="utf-8") as f:
            all_texts[path] = f.read()

    else:
        click.echo("è«‹æä¾› .md æª”æ¡ˆæˆ–åŒ…å« .md æª”æ¡ˆçš„è³‡æ–™å¤¾", err=True)
        return

    import_md_notes_flow(all_texts)


if __name__ == "__main__":
    cli()
# python ingest.py --file /ingest/'Chapter 1_ Big Data.md'
