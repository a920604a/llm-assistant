import requests
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from qdrant_client import QdrantClient
from qdrant_client.http import models as rest
import os

# âœ… è¨­å®š API é‡‘é‘°èˆ‡æœå‹™ç«¯é»
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
OLLAMA_API_URL = "http://localhost:11434"

# âœ… åŸºæœ¬è¨­å®š
collection_name = "notes_collection"

# âœ… Langchain è¨­å®š
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
embeddings = OpenAIEmbeddings()

# âœ… Qdrant åˆå§‹åŒ–
qdrant_client = QdrantClient(url="http://localhost:6333")

if collection_name not in [c.name for c in qdrant_client.get_collections().collections]:
    qdrant_client.recreate_collection(
        collection_name=collection_name,
        vectors_config=rest.VectorParams(size=1536, distance=rest.Distance.COSINE),
    )


# âœ… ç¿»è­¯æ–‡å­—
def ollama_translate(text: str) -> str:
    response = requests.post(
        f"{OLLAMA_API_URL}/api/generate",
        json={
            "model": "llama3",  # æˆ–æ‚¨æœ‰è‡ªå·±å‘½åçš„ç¿»è­¯æ¨¡å‹
            "prompt": f"è«‹å¹«æˆ‘å°‡ä»¥ä¸‹å…§å®¹ç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼š\n{text}",
            "stream": False,
        },
    )
    response.raise_for_status()
    return response.json()["response"]


# âœ… ç”¢å‡ºæ•™å­¸ç­†è¨˜çš„ Metadata
def ollama_generate_metadata(text: str) -> dict:
    meta_prompt = f"""
è«‹é–±è®€ä»¥ä¸‹æ•™å­¸ç­†è¨˜ï¼Œä¸¦åˆ†æå‡ºä»¥ä¸‹å±¬æ€§ï¼š
- ä¸»é¡Œåˆ†é¡ï¼ˆä»¥ä¸€æ®µæ–‡å­—æè¿°ï¼‰
- é©åˆç¨‹åº¦ï¼ˆåˆå­¸è€…ã€ä¸­éšã€é€²éšï¼‰
- ä¸‰å€‹é—œéµå­—

æ•™å­¸å…§å®¹å¦‚ä¸‹ï¼š
{text}
è«‹ç”¨ JSON æ ¼å¼è¼¸å‡ºï¼Œä¾‹å¦‚ï¼š
{{
  "topic": "å¤§æ•¸æ“šæ¶æ§‹è¨­è¨ˆ",
  "level": "ä¸­éš",
  "keywords": ["æ•¸æ“šè½‰å‹", "åˆ†æ•£å¼å„²å­˜", "è³‡æ–™æ¹–"]
}}
"""
    response = requests.post(
        f"{OLLAMA_API_URL}/api/generate",
        json={
            "model": "llama3",
            "prompt": meta_prompt,
            "stream": False,
        },
    )
    response.raise_for_status()
    raw = response.json()["response"]

    # å˜—è©¦è§£æ JSON
    try:
        import json
        return json.loads(raw)
    except Exception:
        print("âš ï¸ ç„¡æ³•è§£æ metadata JSONï¼Œå›å‚³ç‚ºï¼š", raw)
        return {"topic": "", "level": "", "keywords": []}


# âœ… åŒ¯å…¥ Markdown ç­†è¨˜ï¼ˆè‡ªå‹•ç¿»è­¯ + Metadata + åˆ‡å‰² + å¯«å…¥ Qdrantï¼‰
def import_md_notes(md_text_list):
    points = []
    idx = 0

    for md_text in md_text_list:
        print(f"â¡ï¸ ç¿»è­¯ä¸­ï¼ˆåŸå§‹å­—æ•¸: {len(md_text)}ï¼‰...")
        translated = ollama_translate(md_text)

        print("ğŸ§  ç”¢å‡º Metadata...")
        metadata = ollama_generate_metadata(translated)

        print("âœ‚ï¸ åˆ†æ®µä¸­...")
        chunks = text_splitter.split_text(translated)
        for chunk in chunks:
            vector = embeddings.embed_query(chunk)
            payload = {
                "text": chunk,
                "translated": True,
                **metadata,
            }
            points.append(
                rest.PointStruct(id=idx, vector=vector, payload=payload)
            )
            idx += 1

    print(f"ğŸ“¦ å¯«å…¥ {len(points)} ç­†è³‡æ–™åˆ° Qdrant")
    qdrant_client.upsert(collection_name=collection_name, points=points)


# âœ… æ¸¬è©¦åŒ¯å…¥ç¯„ä¾‹
# md_notes = [
#     """
# # Chapter 1: Big Data
# To do that, you need a data architecture to ingest, store, transform, and model the data so it can be accurately and easily used.
# ## What Is Big Data?
# Big Data is defined by Volume, Variety, Velocity...
# """
# ]

import_md_notes(md_notes)
